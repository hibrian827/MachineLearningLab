{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6j5WcA3GbSk"
   },
   "source": [
    "# M2608.002900 기계학습 기초 <br> Assignment 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9SLZYvrD1cm"
   },
   "source": [
    "# Setup\n",
    "Check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn 0.20 or later is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required library (might take a while)\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RABKSi6ED1cm"
   },
   "outputs": [],
   "source": [
    "# Python >=3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn >=0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei9BMeksGbSn"
   },
   "source": [
    "## Dataset load & Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vlb19coAGbSo"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xV9Z9RPMGbSt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.csv', delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "label_mask = np.equal(y, 1)\n",
    "\n",
    "plt.scatter(X[:, 0][label_mask], X[:, 1][label_mask], color='red')\n",
    "plt.scatter(X[:, 0][~label_mask], X[:, 1][~label_mask], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwcWe3pDGbSy"
   },
   "source": [
    "## Problem 0-1. sklearn model로 Logistic Regression 모델 train 시켜보기\n",
    "scikit-learn library의 LogisticRegression 클래스를 이용해 train 시켜 보세요. <br>\n",
    "클래스 인자 및 사용법에 관한 자세한 설명은 scikit-learn 홈페이지의 설명을 참고해 주세요. <br>\n",
    "(참고: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "코드 구현에 LLM(Chat-GPT, Gemini, GROK 등)을 보조 도구로 활용하여도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONWQ_Q5yGbS0"
   },
   "outputs": [],
   "source": [
    "def learn_and_return_weights(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    # YOUR CODE COMES HERE\n",
    "    # w: coefficient of the model to input features,\n",
    "    # b: bias of the model\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkQB55lkGbS3"
   },
   "outputs": [],
   "source": [
    "def plot_data_and_weights(X, y, w, b):\n",
    "    plt.scatter(X[:, 0][label_mask], X[:, 1][label_mask], color='red')\n",
    "    plt.scatter(X[:, 0][~label_mask], X[:, 1][~label_mask], color='blue')\n",
    "\n",
    "    x_lin = np.arange(20, 70)\n",
    "    y_lin = -(b + w[0] * x_lin) / w[1]\n",
    "\n",
    "    plt.plot(x_lin, y_lin, color='black');\n",
    "    plt.show()\n",
    "\n",
    "w, b = learn_and_return_weights(X, y)\n",
    "plot_data_and_weights(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKYz83ecGbS6"
   },
   "source": [
    "## Problem 0-2. numpy로 Logistic Regression 구현해보기\n",
    "scikit-learn library를 사용하지 않고 numpy를 활용하여 Logistic Regression을 구현해보세요.\n",
    "\n",
    "코드 구현에 LLM(Chat-GPT, Gemini, GROK 등)을 보조 도구로 활용하여도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGTRLxvsGbS7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    # YOUR CODE COMES HERE\n",
    "    return\n",
    "\n",
    "def binary_cross_entropy_loss(y_pred: np.ndarray, target: np.ndarray) -> float:\n",
    "    # YOUR CODE COMES HERE\n",
    "    return\n",
    "\n",
    "def learn_and_return_weights_numpy(X: np.ndarray, Y: np.ndarray, lr: float = 0.01, iter: int = 100000) -> tuple[np.ndarray, float]:\n",
    "    # YOUR CODE COMES HERE\n",
    "    # w: coefficient of the model to input features,\n",
    "    # b: bias of the model\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryO_ItqRGbS_"
   },
   "outputs": [],
   "source": [
    "w, b = 0, 0\n",
    "w, b = learn_and_return_weights_numpy(X, y)\n",
    "plot_data_and_weights(X, y, w, b)\n",
    "\n",
    "z = np.dot(X, w) + b\n",
    "y_output = sigmoid(z)\n",
    "bce = binary_cross_entropy_loss(y_output,y)\n",
    "if np.isnan(bce) == True:\n",
    "    print('You need to make sure your binary cross entropy loss function is correct,\\nor add a small number (e.g. 1e-10) to the argument of the logarithm to make sure the argument of the logarithm is not zero.')\n",
    "else:\n",
    "    print('Binary cross entropy loss:', bce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "v_DaRCDGHJZ5"
   },
   "source": [
    "## Problem 1-1. Logistic Regression으로 multi-class classification 하기: API 활용하기\n",
    "scikit-learn library의 Logistic Regression API를 활용하면 multi-class classification을 간단하게 수행할 수 있습니다.<br>\n",
    "MNIST dataset에 대해 multi-class classification을 위한 Logistic Regression 모델을 학습시키고, test data에 대한 accuracy를 계산해 보세요.\n",
    "\n",
    "**2개 이상의 LLM을 활용하여 코드를 구현하고, 구현 결과를 비교 분석하여 보고서에 작성하세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJNyQWnuGbTL"
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    from torchvision.datasets import MNIST\n",
    "    train_data = MNIST(root = './data', train=True, download=True)\n",
    "    test_data = MNIST(root = './data', train=False, download=True)\n",
    "    x_train = train_data.data.numpy()\n",
    "    y_train = train_data.targets.numpy()\n",
    "    x_test = test_data.data.numpy()\n",
    "    y_test = test_data.targets.numpy()\n",
    "    x_train = x_train.reshape((-1, 28 * 28)).astype(np.float32)\n",
    "    x_test = x_test.reshape((-1, 28 * 28)).astype(np.float32)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "(x_train, y_train), (x_test, y_test) = get_dataset()\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Hi81olzGbTT"
   },
   "outputs": [],
   "source": [
    "def learn_mul(X: np.ndarray, y: np.ndarray) -> LogisticRegression:\n",
    "    ################# YOUR CODE COMES HERE ######################\n",
    "    # training and return the multi-class logistic model\n",
    "\n",
    "    #############################################################\n",
    "    return lr\n",
    "\n",
    "def inference_mul(x: np.ndarray, lr_model: LogisticRegression) -> np.ndarray:\n",
    "    ################# YOUR CODE COMES HERE ######################\n",
    "    # inference model and return predicted y values\n",
    "\n",
    "    #############################################################\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfdTZNUYGbTW"
   },
   "outputs": [],
   "source": [
    "model = learn_mul(x_train, y_train)\n",
    "preds = inference_mul(x_test, model)\n",
    "accuracy = np.sum(preds == y_test) / y_test.shape[0]\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "2OLcsStrGbTK"
   },
   "source": [
    "## Problem 1-2. Logistic Regression으로 multi-class classification 하기: Transformation to Binary\n",
    "\n",
    "Logistic Regression은 기본적으로 binary classifier 입니다. 즉, input *X*를 2개의 class로 밖에 분류하지 못합니다.<br>\n",
    "하지만, 이같은 Logistic Regression 모델을 연달아 사용한다면 data를 여러 class로 분류할 수도 있습니다.<br>\n",
    "(참고: https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary)\n",
    "\n",
    "MNIST dataset을 이용하여 (class 수) 개의 Binary classifier (Logistic Regression)를 'lrs'의 각 원소에 저장한 뒤,<br>\n",
    "학습시킨 모델들을 이용하여 test data에 대한 accuracy를 계산해 보세요.<br>\n",
    "(각 모델의 training iteration은 10회면 충분합니다.)\n",
    "\n",
    "**2개 이상의 LLM을 활용하여 코드를 구현하고, 구현 결과를 비교 분석하여 보고서에 작성하세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlYDigjwGbTO"
   },
   "outputs": [],
   "source": [
    "def learn_mul2bin(X: np.ndarray, y: np.ndarray, num_classes: int) -> list[LogisticRegression]:\n",
    "    lrs = []\n",
    "    ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n/10%10!=1)*(n%10<4)*n%10::4])\n",
    "    for i in range(num_classes):\n",
    "        print('training %s classifier'%(ordinal(i+1)))\n",
    "        ################# YOUR CODE COMES HERE ######################\n",
    "        # training and return the multi-class logistic model\n",
    "\n",
    "        #############################################################\n",
    "\n",
    "    return lrs\n",
    "\n",
    "def inference_mul2bin(X: np.ndarray, lrs: list[LogisticRegression]) -> int:\n",
    "    ################# YOUR CODE COMES HERE ######################\n",
    "    # inference model and return predicted y values\n",
    "\n",
    "    #############################################################\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKF3C-4OGbTR"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "models = learn_mul2bin(x_train, y_train, num_classes)\n",
    "preds = np.array([inference_mul2bin(x, models) for x in x_test])\n",
    "accuracy = np.sum(preds == y_test) / y_test.shape[0]\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZU2mqKXEkVf"
   },
   "source": [
    "## Problem 2. Logistic Regression을 활용한 임의의 데이터셋 분류 실습\n",
    "\n",
    "**이번 문제에서는 seaborn 라이브러리의 titanic dataset을 활용하여 분류를 실습해볼 것입니다.** titanic dataset은 1912년 타이타닉 호 침몰 사고 당시 승객들의 정보를 담고 있으며, 생존 여부를 비롯한 다양한 특성으로 구성된 데이터셋입니다. 해당 데이터셋에 존재하는 승객들의 다양한 특성들을 활용하여 승객의 생존 여부를 logistic regression 알고리즘으로 분류하는 실습을 진행해보겠습니다.\n",
    "\n",
    "자세히 설명하자면, 승객의 성별(sex), 나이(age), 탑승 요금(fare), 객실 등급(class), 함께 탑승한 가족 구성원 수(sibsp, parch), 탑승한 도시(embark_town), 혼자 탑승했는 지 여부(alone)을 활용하여 생존 여부(survived)를 예측합니다.  \n",
    "데이터셋에 대한 추가적인 설명은 아래 링크를 참조하세요.   \n",
    "(참고: https://www.kaggle.com/c/titanic/data)\n",
    "\n",
    "앞의 실습 내용들을 참고하여 numpy로 여러 변수에 대한 logistic regression을 구현하세요.  \n",
    "LLM을 도구로 활용하여 구현하여도 괜찮습니다.  \n",
    "\n",
    "**※ 수업 시간에 배운 logistic regression의 성능을 올리기 위한 여러 방식들을 활용하여 최대한 성능을 높여보세요.**  \n",
    "(ex. L1, L2 regularization, polynomial features 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nYKfA4tFAQb"
   },
   "outputs": [],
   "source": [
    "################################### Dataset Preprocessing #############################################\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# 데이터 확인\n",
    "print(df.head())\n",
    "\n",
    "df = df[['sex', 'age', 'fare', 'class', 'sibsp', 'parch', 'embark_town', 'alone', 'survived']].dropna()\n",
    "\n",
    "# 범주형 데이터 인코딩\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})   # 성별 숫자로 변환\n",
    "df['class'] = df['class'].map({'First': 1, 'Second': 2, 'Third': 3})  # 좌석 등급 숫자로 변환\n",
    "df['alone'] = df['alone'].astype(int)  # 혼자 탑승 여부 (Boolean → int 변환)\n",
    "\n",
    "# 원핫 인코딩 (탑승 도시)\n",
    "df = pd.get_dummies(df, columns=['embark_town'], drop_first=True)\n",
    "\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roG_4gRnhCqO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 분할 (80% 학습, 20% 테스트)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "error",
     "timestamp": 1740319207067,
     "user": {
      "displayName": "Cody Song",
      "userId": "05250416816858502527"
     },
     "user_tz": -540
    },
    "id": "Qh08tR7QEkkD",
    "outputId": "10fec2c1-6f0f-47ad-ee02-d0928105243f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, ): # <<< You can add your own input parameters\n",
    "        ################# YOUR CODE COMES HERE ######################\n",
    "        # initialize class member variable\n",
    "\n",
    "        #############################################################\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # YOUR CODE COMES HERE\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ################# YOUR CODE COMES HERE ######################\n",
    "        # training model here\n",
    "\n",
    "        #############################################################\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        ################# YOUR CODE COMES HERE ######################\n",
    "        # return predicted y\n",
    "\n",
    "        #############################################################\n",
    "        return\n",
    "\n",
    "    # You can add your own member functions\n",
    "\n",
    "model = LogisticRegression( ) ##### ADD YOUR OWN INPUT PARAMETERS\n",
    "\n",
    "# 모델 학습 및 예측\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 정확도 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"모델 정확도: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": [
    {
     "file_id": "1br9I49jKldloi3aOJ6HPGtq3RLU9h5ft",
     "timestamp": 1583728216996
    }
   ]
  },
  "kernelspec": {
   "display_name": "machine-learning-23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0b5d2b764069d04ae05c697c65ef1186248c0f69c25ae497443bad4761b2ec0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
